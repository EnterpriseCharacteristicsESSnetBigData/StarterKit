{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Scraper Starter Kitt\n",
    "## Structure of this Starter Kitt\n",
    "\n",
    "1. Source code (in Python) - library and application\n",
    "2. Jupyter Notebook files (ipynb) including manuals inside\n",
    "3. Example files - data with urls - url.txt\n",
    "\n",
    "### Data processing schema\n",
    "URL list in files -> URLScraper -> Websites in NoSQL collections for further processing\n",
    "\n",
    "### Prerequisites\n",
    "Create a file url.txt with the following structure (one row for one url):\n",
    "\n",
    "http://stat.gov.pl\n",
    "\n",
    "http://destatis.de\n",
    "\n",
    "http://www.nsi.bg\n",
    "\n",
    "Five steps to run this application.\n",
    "\n",
    "1. Import libraries\n",
    "2. Create a connection to mongodb server\n",
    "3. Set the database name\n",
    "4. Set the file name of URLs to import\n",
    "5. Start the web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries.\n",
    "If they do not exist please update your Python environment with pip, pip3, conda or easy_install. Look into manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import string, sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a connection to mongodb server. \n",
    "\n",
    "Replace the values below with your own.\n",
    "\n",
    "### Variables to set:\n",
    "\n",
    "servername - change with IP address or name of the server, e.g. 192.168.1.1 or serverdb.domain.com\n",
    "\n",
    "port - change the port number - for MongoDB default is 27017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='localhost'\n",
    "port=27017\n",
    "# define the client connection\n",
    "# host - default localhost\n",
    "# port - default 27017\n",
    "client=MongoClient('mongodb://'+str(host)+\":\"+str(port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set the database name.\n",
    "\n",
    "### Variable to set:\n",
    "\n",
    "dbname - if the database does not exist it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname='wpc'\n",
    "try:\n",
    "    database=client[dbname]\n",
    "except:\n",
    "    print('Error connecting the database', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Set the file name of URLs to import. \n",
    "\n",
    "The file should be structured like this:\n",
    "\n",
    "http://stat.gov.pl\n",
    "\n",
    "http://destatis.de\n",
    "\n",
    "http://www.nsi.bg\n",
    "\n",
    "### Variable to set:\n",
    "\n",
    "filename - the name of the file, e.g. url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='url.txt'\n",
    "file=open(filename,'r') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Start the web scraping.\n",
    "\n",
    "### Variables to set:\n",
    "\n",
    "timeBetweenRequests - set the time between requests - in seconds (suggested 3-5 seconds).\n",
    "\n",
    "collectionName - default database.websites - value after dot can be changed, e.g. database.myfirstcollection, database.wpc_20200301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping  http://maslankowski.pl\n",
      "Scrapped  http://maslankowski.pl\n",
      "Scrapping  http://destatis.de\n",
      "Scrapped  http://destatis.de\n"
     ]
    }
   ],
   "source": [
    "timeBetweenRequests=5\n",
    "collectionName = database.websites\n",
    "for url in file:\n",
    "    print('Scrapping ',url.strip())\n",
    "    website=requests.get(url.strip())    \n",
    "    json = {\n",
    "        'url': str(url.strip()),\n",
    "        'content':  website.text,\n",
    "        'date': str(datetime.now())\n",
    "    }\n",
    "    result = collectionName.insert_one(json)\n",
    "    print('Scrapped ',url.strip())\n",
    "    # N second delay on purpose\n",
    "    time.sleep(timeBetweenRequests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
