{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Scraper Starter Kit\n",
    "## Structure of this Starter Kit\n",
    "\n",
    "1. Source code (in Python) - library and application\n",
    "2. Jupyter Notebook files (ipynb) including manuals inside\n",
    "3. Example files - data with urls - url.txt\n",
    "\n",
    "### Data processing schema\n",
    "URL list in files -> URLScraper -> Websites in NoSQL collections for further processing\n",
    "\n",
    "### Prerequisites\n",
    "A data source containing the URLs to scrape is needed. It can be an iterable like a list, or a data frame column containing URLs. For this starer kit, we use an input file that is line-separated and looks like this:\n",
    "\n",
    "http://stat.gov.pl\n",
    "\n",
    "http://destatis.de\n",
    "\n",
    "http://www.nsi.bg\n",
    "\n",
    "Five steps to run this application.\n",
    "\n",
    "1. Import libraries\n",
    "2. Create a connection to mongodb server\n",
    "3. Set the database name\n",
    "4. Set the file name of URLs to import\n",
    "5. Start the web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries.\n",
    "If they do not exist please update your Python environment with pip, pip3, conda or easy_install. Look into manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a connection to mongodb server. \n",
    "\n",
    "Replace the values below with your own.\n",
    "\n",
    "### Variables to set:\n",
    "\n",
    "servername - change with IP address or name of the server, e.g. 192.168.1.1 or serverdb.domain.com\n",
    "\n",
    "port - change the port number - for MongoDB default is 27017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='localhost'\n",
    "port=27017\n",
    "# define the client connection\n",
    "# host - default localhost\n",
    "# port - default 27017\n",
    "client=MongoClient('mongodb://'+str(host)+\":\"+str(port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set the database name.\n",
    "\n",
    "### Variable to set:\n",
    "\n",
    "dbname - if the database does not exist it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname='URLScraping'\n",
    "try:\n",
    "    database=client[dbname]\n",
    "except:\n",
    "    print('Error connecting the database', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import the file containing URLs to scrape. \n",
    "We created a line separated file containing URLs as explained in the prerequisites.\n",
    "\n",
    "### Variable to set:\n",
    "\n",
    "filename - the name of the file, e.g. url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='url.txt'\n",
    "file=open(filename,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define Class and methods for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeDomain():\n",
    "    ##################\n",
    "    # init\n",
    "    ##################\n",
    "    def __init__(self, domain, lang_codes=[], max_pages=1, accept_subdomains=False):\n",
    "        self.domain = domain.lower().strip().replace('http://','').replace('https://','').replace('www.','')\n",
    "        self.domain_link = 'https://'+self.domain\n",
    "        self.json = {'domain': self.domain,\n",
    "                     'content': {}} # All scraped pages are embedded documents within content dict\n",
    "        self.max_pages = max_pages\n",
    "        self.link_set = {self.domain_link}\n",
    "        self.num_pages = 0\n",
    "        self.scraped = set()\n",
    "        self.accept_subdomains = accept_subdomains\n",
    "        self.lang_codes = lang_codes\n",
    "        if self.lang_codes:\n",
    "            self.lang_idents = []\n",
    "            #TODO: language tag identification with regular expressions\n",
    "            for language in self.lang_codes:\n",
    "                self.lang_idents.append(\"/{}/\".format(language))\n",
    "                self.lang_idents.append(\"/{}-{}/\".format(language, language))\n",
    "                self.lang_idents.append(\"?lang={}\".format(language))\n",
    "\n",
    "    \n",
    "    ##################\n",
    "    # Utility functions\n",
    "    ##################\n",
    "    # Exclude downloadable files, pictures, etc from being scraped\n",
    "    # List taken from ARGUS by datawizard1337 (I added xls and xlsx)\n",
    "    # https://github.com/datawizard1337/ARGUS --> language prioritising was also inspired by ARGUS\n",
    "    filetypes = set(['mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif', 'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg',\n",
    "            'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff',\n",
    "            '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv', 'm4a',\n",
    "            'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar', 'msu', 'flv', 'dmg', 'xls', 'xlsx',\n",
    "            'mng?download=true', 'pct?download=true', 'bmp?download=true', 'gif?download=true', 'jpg?download=true', 'jpeg?download=true', 'png?download=true', 'pst?download=true', 'psp?download=true', 'tif?download=true', 'tiff?download=true', 'ai?download=true', 'drw?download=true', 'dxf?download=true', 'eps?download=true', 'ps?download=true', 'svg?download=true',\n",
    "            'mp3?download=true', 'wma?download=true', 'ogg?download=true', 'wav?download=true', 'ra?download=true', 'aac?download=true', 'mid?download=true', 'au?download=true', 'aiff?download=true',\n",
    "            '3gp?download=true', 'asf?download=true', 'asx?download=true', 'avi?download=true', 'mov?download=true', 'mp4?download=true', 'mpg?download=true', 'qt?download=true', 'rm?download=true', 'swf?download=true', 'wmv?download=true', 'm4a?download=true',\n",
    "            'css?download=true', 'pdf?download=true', 'doc?download=true', 'exe?download=true', 'bin?download=true', 'rss?download=true', 'zip?download=true', 'rar?download=true', 'msu?download=true', 'flv?download=true', 'dmg?download=true'])\n",
    "\n",
    "    def det_prio(self):\n",
    "        '''Determines which URL should be scraped next'''\n",
    "        not_scraped = list(self.link_set.difference(self.scraped))\n",
    "        if not self.lang_codes:\n",
    "            link_stack = sorted(not_scraped, key=len)\n",
    "        else:\n",
    "            correct_lang = []\n",
    "            other_lang = []\n",
    "            for link in not_scraped:\n",
    "                if any(tag.lower() in link.lower() for tag in self.lang_idents):\n",
    "                    correct_lang.append(link)\n",
    "                else:\n",
    "                    other_lang.append(link)\n",
    "            # Sort urls that were not yet scraped by link length\n",
    "            link_stack = sorted(correct_lang, key=len) + sorted(other_lang, key=len)\n",
    "        self.to_scrape = link_stack[0]\n",
    "    def extractLinks(self):      \n",
    "        soup = BeautifulSoup(self.website.text,\"html.parser\")\n",
    "        for a in soup.find_all('a', href=True):\n",
    "           url = self.get_internalURL(a['href'])\n",
    "           if url:\n",
    "               # only include links that don't belong to blacklisted filetypes and not mailto links\n",
    "               if not url.split(\".\")[-1].lower() in self.filetypes:\n",
    "                   self.link_set.add(url)\n",
    "    def get_internalURL(self, url):\n",
    "        #ignore javascript, mailto and telephone links\n",
    "        pattern = re.compile(\"mailto:|tel:|javascript:\", re.IGNORECASE)\n",
    "        if url and not re.search(pattern, url):\n",
    "            if self.domain in url:\n",
    "                if self.accept_subdomains == False:\n",
    "                    # Test whether link doesn't contain a subdomain\n",
    "                    cleaned_url = url.lower().replace('http://','').replace('https://','').replace('www.','')\n",
    "                    if cleaned_url.split('.')[0]==self.domain.split('.')[0]:\n",
    "                        return url\n",
    "                else:\n",
    "                    return url\n",
    "            elif url[0:2]==\"./\":\n",
    "                return self.domain_link+url.replace('./','/')\n",
    "            elif url[0]==\"/\":\n",
    "                return self.domain_link+url\n",
    "            elif \"http\" not in url:\n",
    "                return self.domain_link+'/'+url\n",
    "\n",
    "    ##################\n",
    "    # Scraper\n",
    "    ##################\n",
    "    def url_scraping(self, user_agent, timeOutConnect=10,\n",
    "                     timeOutRead=15, timeBetweenRequests=2):\n",
    "        '''Scrapes link'''\n",
    "        self.det_prio()\n",
    "        print('Scraping', self.to_scrape)\n",
    "        headers = {'user-agent': user_agent}\n",
    "        self.scraped.add(self.to_scrape)\n",
    "        try:\n",
    "            self.website=requests.get(self.to_scrape, headers=headers,\n",
    "                                      timeout=(timeOutConnect,timeOutRead))\n",
    "            self.num_pages += 1\n",
    "            self.scraped.add(self.website.url) # Add scraped url to scraped set (in case of redirect)\n",
    "            print('Success in scraping page no', self.num_pages, 'of Domain:', self.domain)\n",
    "            self.json['content'].update({str(self.num_pages): {'url': self.website.url,\n",
    "                                                          'page': self.website.text,\n",
    "                                                          'date': str(datetime.now())}})\n",
    "            # I use website.url to obtain the url that was actually scraped \n",
    "            # (different to original url in case of redirects)\n",
    "            if self.num_pages < self.max_pages:\n",
    "                self.extractLinks()\n",
    "            if self.num_pages >= self.max_pages or not self.link_set.difference(self.scraped):\n",
    "                # Save json to MongoDB when max_pages is reached or no links are left for scraping\n",
    "                try:\n",
    "                    result = collectionName.insert_one(self.json)\n",
    "                    print('Saved scraped pages to database')\n",
    "                except:\n",
    "                    print('Error while saving into database occurred')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print('Error scraping', self.to_scrape)\n",
    "            print(f'Error message: {type(e)}: {e}')\n",
    "        finally:\n",
    "            # Even if the current page produced an error, continue scraping if there are still page links left\n",
    "            if self.link_set.difference(self.scraped) and self.num_pages<self.max_pages:\n",
    "                # N second delay on purpose\n",
    "                time.sleep(timeBetweenRequests)\n",
    "                self.url_scraping(user_agent, timeOutConnect, timeOutRead, timeBetweenRequests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Start the web scraping.\n",
    "\n",
    "### Variables to set:\n",
    "\n",
    "collectionName - default database.websites - value after dot can be changed, e.g. database.myfirstcollection, database.wpc_20200301\n",
    "\n",
    "max_pages - maximum number of pages on domain to be scraped\n",
    "\n",
    "preferred_langs - list of ISO language codes that should be prioritised while scraping (can be omitted)\n",
    "\n",
    "accept_subdomains - if set to True, also allow the crawler to scrape subdomains, which have the format: subdomain.domain --> eg. https://www-genesis.destatis.de/genesis/online, so \"www-genesis\" is a subdomain of destatis.de (this is where you can accesss the database of destatis)\n",
    "\n",
    "userAgent - the name of the robot (should be changed to the name of your organization and the purpose of scraping)\n",
    "\n",
    "timeBetweenRequests - set the time between requests - in seconds (suggested 3-5 seconds).\n",
    "\n",
    "timeOutConnect - maximum time in seconds to connect to the website\n",
    "\n",
    "timeOutRead - maximum time in seconds to read the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Database collection to be used\n",
    "collectionName = database.websites\n",
    "\n",
    "# Parameters for ScrapeDomain class\n",
    "max_pages = 10\n",
    "preferred_langs = ['en']\n",
    "accept_subdomains = False\n",
    "\n",
    "# Parameters for scraping\n",
    "userAgent='python-app/0.1 experimental for statistical purposes'\n",
    "timeBetweenRequests=2\n",
    "timeOutConnect=10\n",
    "timeOutRead=15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://stat.gov.pl\n",
      "Error scraping https://stat.gov.pl\n",
      "Error message: <class 'requests.exceptions.SSLError'>: HTTPSConnectionPool(host='stat.gov.pl', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])\")))\n",
      "Scraping https://destatis.de\n",
      "Success in scraping page no 1 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Home/_node.html\n",
      "Success in scraping page no 2 of Domain: destatis.de\n",
      "Scraping https://destatis.de/Europa/EN/Home/_node.html\n",
      "Success in scraping page no 3 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Service/Reporting-Online/_node.html\n",
      "Success in scraping page no 4 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Themes/Countries-Regions/International-Statistics/_node.html\n",
      "Success in scraping page no 5 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Home/_node.html;jsessionid=2A275C368E8569F427E9580611C84031.internet8711\n",
      "Success in scraping page no 6 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Home/_node.html;jsessionid=2336A732B434DB03DC199F528FB96218.internet8731\n",
      "Success in scraping page no 7 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Home/_node.html;jsessionid=70CA5F524E30EF0892A7618BC5065007.internet8721\n",
      "Success in scraping page no 8 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Press/_node.html\n",
      "Success in scraping page no 9 of Domain: destatis.de\n",
      "Scraping https://destatis.de/EN/Themes/_node.html\n",
      "Success in scraping page no 10 of Domain: destatis.de\n",
      "Saved scraped pages to database\n",
      "Scraping https://nsi.bg\n",
      "Success in scraping page no 1 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/#\n",
      "Success in scraping page no 2 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en\n",
      "Success in scraping page no 3 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/79\n",
      "Success in scraping page no 4 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/108\n",
      "Success in scraping page no 5 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/109\n",
      "Success in scraping page no 6 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/106\n",
      "Success in scraping page no 7 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/5640\n",
      "Success in scraping page no 8 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/5553\n",
      "Success in scraping page no 9 of Domain: nsi.bg\n",
      "Scraping https://nsi.bg/en/node/6316\n",
      "Success in scraping page no 10 of Domain: nsi.bg\n",
      "Saved scraped pages to database\n"
     ]
    }
   ],
   "source": [
    "# Loop through domain urls to scrape\n",
    "\n",
    "for domain in file:\n",
    "    URLScraping = ScrapeDomain(domain, \n",
    "                               max_pages = max_pages,\n",
    "                               lang_codes = preferred_langs,\n",
    "                               accept_subdomains = False)\n",
    "    URLScraping.url_scraping(userAgent, timeOutConnect, timeOutRead, timeBetweenRequests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The language priotising doesn't work as well for nsi.bg, because it expects the language tag to be within two slashes. I should probably change it so that a URL like http://nsi.bg/en is priotised over http://nsi.bg/bg when 'en' is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Retrieve saved website data from MongoDB\n",
    "This part shows how the data is saved in the NoSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first saved website as an example\n",
    "a_website = database.websites.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the HTML code of the first domain and the first page\n",
    "print(a_website['content']['1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
