{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Scraper Starter Kit\n",
    "## Structure of this Starter Kit\n",
    "\n",
    "1. Source code (in Python) - library and application\n",
    "2. Jupyter Notebook files (ipynb) including manuals inside\n",
    "3. Example files - data with urls - url.txt\n",
    "\n",
    "### What does this module do?\n",
    "The URLScraper takes a URL and scrapes it. If specified, internal links up to a specified number are also scraped. When deciding which internal links to scrape, tagged links with a specific language (if defined) and shorter links are prioritized. The URLScraper then saves the scraped data into a NoSQL database (MongoDB).\n",
    "\n",
    "URL list in files -> URLScraper -> Websites in NoSQL collections for further processing\n",
    "\n",
    "### Prerequisites\n",
    "A data source containing the URLs to scrape is needed. It can be an iterable like a list, or a data frame column containing URLs. For this starter kit, we use an input file that is line-separated and looks like this:\n",
    "\n",
    "http://stat.gov.pl\n",
    "\n",
    "http://destatis.de\n",
    "\n",
    "http://www.nsi.bg\n",
    "\n",
    "Five steps to run this application.\n",
    "\n",
    "1. Import libraries\n",
    "2. Create a connection to mongodb server\n",
    "3. Set the database name\n",
    "4. Set the file name of URLs to import\n",
    "5. Start the web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries.\n",
    "If they do not exist please update your Python environment with pip, pip3, conda or easy_install. Look into manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "from DomainScraper import ScrapeDomain\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a connection to mongodb server. \n",
    "\n",
    "Replace the values below with your own.\n",
    "\n",
    "### Variables to set:\n",
    "\n",
    "servername - change with IP address or name of the server, e.g. 192.168.1.1 or serverdb.domain.com\n",
    "\n",
    "port - change the port number - for MongoDB default is 27017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='localhost'\n",
    "port=27017\n",
    "# define the client connection\n",
    "# host - default localhost\n",
    "# port - default 27017\n",
    "client=MongoClient('mongodb://'+str(host)+\":\"+str(port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set the database name.\n",
    "\n",
    "### Variable to set:\n",
    "\n",
    "dbname - if the database does not exist it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname='URLScraping'\n",
    "try:\n",
    "    database=client[dbname]\n",
    "except:\n",
    "    print('Error connecting the database', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import the file containing URLs to scrape. \n",
    "We created a line separated file containing URLs as explained in the prerequisites.\n",
    "\n",
    "### Variable to set:\n",
    "\n",
    "filename - the name of the file, e.g. url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='url.txt'\n",
    "file=open(filename,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Start the web scraping.\n",
    "\n",
    "### Variables to set:\n",
    "\n",
    "collectionName - default database.websites - value after dot can be changed, e.g. database.myfirstcollection, database.wpc_20200301\n",
    "\n",
    "max_pages - maximum number of pages on domain to be scraped\n",
    "\n",
    "preferred_langs - list of ISO language codes that should be prioritised while scraping (can be omitted)\n",
    "\n",
    "accept_subdomains - if set to True, also allow the crawler to scrape subdomains, which have the format: subdomain.domain --> eg. https://www-genesis.destatis.de/genesis/online, so \"www-genesis\" is a subdomain of destatis.de (this is where you can accesss the database of destatis)\n",
    "\n",
    "userAgent - the name of the robot (should be changed to the name of your organization and the purpose of scraping)\n",
    "\n",
    "timeBetweenRequests - set the time between requests - in seconds (suggested 3-5 seconds).\n",
    "\n",
    "timeOutConnect - maximum time in seconds to connect to the website\n",
    "\n",
    "timeOutRead - maximum time in seconds to read the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Database collection to be used\n",
    "collectionName = database.websites\n",
    "\n",
    "# Parameters for ScrapeDomain class\n",
    "max_pages = 1\n",
    "preferred_langs = ['en']\n",
    "accept_subdomains = False\n",
    "\n",
    "# Parameters for scraping\n",
    "userAgent='python-app/0.1 experimental for statistical purposes'\n",
    "timeBetweenRequests=2\n",
    "timeOutConnect=10\n",
    "timeOutRead=15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://stat.gov.pl\n",
      "Error scraping https://stat.gov.pl\n",
      "Error message: <class 'requests.exceptions.SSLError'>: HTTPSConnectionPool(host='stat.gov.pl', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])\")))\n",
      "Scraping https://destatis.de\n",
      "Success in scraping page no 1 of Domain: destatis.de\n",
      "Saved scraped pages to database\n",
      "Scraping https://nsi.bg\n",
      "Success in scraping page no 1 of Domain: nsi.bg\n",
      "Saved scraped pages to database\n"
     ]
    }
   ],
   "source": [
    "# Loop through domain urls to scrape\n",
    "\n",
    "for index, domain in enumerate(file, 1):\n",
    "    URLScraping = ScrapeDomain(domain = domain,\n",
    "                               index = index,\n",
    "                               max_pages = max_pages,\n",
    "                               lang_codes = preferred_langs,\n",
    "                               accept_subdomains = False)\n",
    "    URLScraping.url_scraping(userAgent, collectionName, timeOutConnect, timeOutRead, timeBetweenRequests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Retrieve saved website data from MongoDB\n",
    "This part shows how the data is saved in the NoSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first saved website as an example\n",
    "a_website = database.websites.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"de\">\n",
      "<head>\n",
      "  <base href=\"https://www.destatis.de/\"/>\n",
      "  <meta charset=\"UTF-8\"/>\n",
      "  <title>Startseite  -  Statistisches Bundesamt</title>\n",
      "  <meta name=\"title\" content=\"Startseite\"/>\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=1.5, user-scalable=1\"/>\n",
      "  <meta name=\"generator\" content=\"Government Site Builder\"/>\n",
      "  \n",
      "  \n",
      "  \n",
      "    <meta name=\"keywords\" content=\"Amtliche Statistik, Pressemitteilung, Publikation, Statistik, Statistisches Bundesamt / Deutschland, Tabelle\"/>\n",
      "    <meta name=\"description\" content=\"Internetangebot des Statistischen Bundesamtes mit aktuellen Informationen, Publikationen, Zahlen und Fakten der amtlichen Statistik\"/>\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<meta property=\"og:site_name\" content=\"Statistisches Bundesamt\"/>\n",
      "<meta property=\"og:type\" content=\"website\"/>\n",
      "<meta property=\"og:title\" content=\"Startseite\"/>\n",
      "<meta property=\"og:description\" content=\"Internetangebot des Statistischen Bundesamtes mit aktuellen Informationen, Publi\n"
     ]
    }
   ],
   "source": [
    "# Print the HTML code of the first domain and the first page\n",
    "print(a_website['content']['1']['page'][:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
